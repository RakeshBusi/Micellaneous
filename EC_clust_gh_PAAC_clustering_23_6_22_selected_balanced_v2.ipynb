{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using PseAAC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u> Part 1: Extracting protein sequence data </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class CAZy_data:\n",
    "    def __init__(self,filename1,filename2):\n",
    "        self.data,self.acc,self.seq=[],[],[]\n",
    "        with open(filename1,'r',encoding='utf-8') as inpt:\n",
    "            for each in inpt:\n",
    "                self.data.append(each.rstrip().split('$'))\n",
    "        with open(filename2,'r',encoding='utf-8') as inpt1:\n",
    "            for each1 in inpt1:\n",
    "                if each1.startswith('>'):\n",
    "                    self.acc.append(each1.rstrip())\n",
    "                else:\n",
    "                    self.seq.append(each1.rstrip())\n",
    "            \n",
    "                  \n",
    "    def data_fetch(self,typ,position):\n",
    "        typ_data=[]\n",
    "        if typ=='all':\n",
    "            typ_data=self.data\n",
    "        else:\n",
    "            for each in self.data:\n",
    "                mult=each[position].split(' ')\n",
    "                if len(mult)==1:#### In case typ = EC, Multi EC number and protein with no EC number are ignore.\n",
    "                    if mult[0]==typ:\n",
    "                        typ_data.append(each)\n",
    "        return typ_data\n",
    "    \n",
    "    def EC_GH(self,ec_no,gh_fam):\n",
    "        self.fasta=[]\n",
    "        cazy_ec=self.data_fetch(ec_no,1)\n",
    "        cazy_gh=self.data_fetch(gh_fam,-1)\n",
    "        self.common_data=[[i[0],i[1],i[3],i[4],i[-2],i[-1]] for i in cazy_ec if i in cazy_gh]\n",
    "        rm_prt, rm_prt_fasta=[],[]\n",
    "        for each in range(len(self.common_data)):\n",
    "            t=self.common_data[each]\n",
    "            if self.prtn_filter(t[0]):\n",
    "                all_acc=t[3].split(' ')\n",
    "                if all_acc[0]!='':\n",
    "                    for e_acc in all_acc:\n",
    "                        e_seq=self.seq_fetch(e_acc)\n",
    "                        try:\n",
    "                            create_error=0/len(e_seq) # to remove accession number which doesnt have hits\n",
    "                            self.fasta.append(f'>{e_acc}${t[0]}${t[1]}${t[2]}${t[-2]}${t[-1]}')\n",
    "                            self.fasta.append(e_seq[0])\n",
    "                        except ZeroDivisionError:\n",
    "                            rm_prt_fasta.append(e_acc)\n",
    "            else:\n",
    "                rm_prt.append(t)\n",
    "#         print('Total number of sequences:',len(self.fasta)/2)\n",
    "#         print('Number of removed partial or fragment proteins (CAZy):',len(rm_prt))\n",
    "#         print('Number of removed partial or fragment proteins (Fasta):',len(rm_prt_fasta))\n",
    "        return self.fasta,rm_prt,rm_prt_fasta\n",
    "    def prtn_filter(self,prt_name):\n",
    "        hit=1\n",
    "        if re.search('partial|fragment',prt_name.lower()):\n",
    "            hit-=1\n",
    "        return hit\n",
    "            \n",
    "    def seq_fetch(self,accession):\n",
    "        hits=[]\n",
    "        temp=0\n",
    "        for each in range(len(self.acc)):\n",
    "            if re.search(f'{accession}\\D',self.acc[each]):\n",
    "                temp+=1\n",
    "                \n",
    "                if self.prtn_filter(self.acc[each]): # remove partial| fragment accession numbers from GenBank description\n",
    "                    hits.append(self.seq[each])\n",
    "        if temp>1:\n",
    "            print(f'Multiple hits for {accession}')\n",
    "        elif temp==0:\n",
    "            print(f'No hits for {accession}')\n",
    "        return hits\n",
    "     \n",
    "In_data=CAZy_data('D:/After_4_4_22/Project_1_Part_1/CAZy_23_6_22/cazy_char_10_6_22.txt','D:/After_4_4_22/Project_1_Part_1/CAZy_23_6_22/char_gh_23_6_22.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u> Part 2: Extracting feature from protein sequences </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# 1: Hydrophobicity, 2: Hydrophilicity, 3: mass, 4: pk1, 5:pk2, 6:pi, 20: 14 scale, 60: Tanford\n",
    "class Standard_values:\n",
    "    def __init__(self,filename):\n",
    "        self.data=[]\n",
    "        with open(filename,'r') as inpt:\n",
    "            for each in inpt:\n",
    "                self.data.append(each.rstrip().split(','))\n",
    "        del self.data[0]\n",
    "        \n",
    "    def get_prop(self,prop):\n",
    "        got_prop,amino_acid={},{}\n",
    "        for each in prop:\n",
    "            got_prop[each]=self.properties(each)\n",
    "        d=1\n",
    "        for each in got_prop[1].keys():\n",
    "            amino_acid[d]=each\n",
    "            d+=1\n",
    "        return amino_acid,got_prop\n",
    "    \n",
    "    def properties(self, val):\n",
    "        temp,norm={},{}\n",
    "        for each in self.data:\n",
    "            temp[each[0]]=float(each[val])\n",
    "        relative=stats.zscore(np.array(list(temp.values())))\n",
    "        for a,b in zip(temp.keys(),relative):\n",
    "            norm[a]=b\n",
    "        return norm\n",
    "\n",
    "class Sequence:\n",
    "    def __init__(self,filename):\n",
    "        self.data=[]\n",
    "        if type(filename)==str:\n",
    "            with open(filename,'r') as inpt:\n",
    "                for each in inpt:\n",
    "                    self.data.append(each.rstrip())\n",
    "        else:\n",
    "            self.data=filename\n",
    "                       \n",
    "    def output(self):\n",
    "        a,s,l=[],[],[]\n",
    "        unusual=0\n",
    "        no_dup,dup=self.remove_dup(self.ml_sl())\n",
    "        sig_fam,mlti=self.multi_fam(no_dup)\n",
    "        for ele in sig_fam:\n",
    "            if ele.startswith('>'):\n",
    "                a.append(ele)\n",
    "            else:\n",
    "                if re.search('[UZOBJX]',ele.upper()):\n",
    "#                     print(a[-1])\n",
    "                    del a[-1]\n",
    "                    unusual+=1\n",
    "                    continue\n",
    "                l.append(len(ele))\n",
    "                s.append(ele.upper())\n",
    "#         print('The length of the smallest sequence:',min(l))\n",
    "#         print('Sequence with \"X\" present:',unusual)\n",
    "        return a,s,l,unusual,dup,mlti\n",
    "    \n",
    "    def multi_fam(self,duta): # remove the same accession number present in multiple families\n",
    "        temp=defaultdict(list)\n",
    "        for ii in range(0,len(duta),2):\n",
    "            name=duta[ii].split('$')[0][1:]\n",
    "            temp[name].append([duta[ii],duta[ii+1]])\n",
    "            \n",
    "        new_duta,mut=[],[]\n",
    "        for jj,kk in dict(temp).items():\n",
    "            if len(kk)!=1:\n",
    "                mut.append(jj)\n",
    "            else:\n",
    "                new_duta.append(kk[0][0])\n",
    "                new_duta.append(kk[0][1])\n",
    "        return new_duta,mut\n",
    "\n",
    "    def remove_dup(self,datu): # remove the duplicates \n",
    "        temp_acc,temp_seq,temp_acc_seq=[],[],[]\n",
    "        dup=[]  \n",
    "        for iii in range(0,len(datu),2):\n",
    "            if datu[iii] not in temp_acc:\n",
    "                temp_acc.append(datu[iii])\n",
    "                temp_acc_seq.append(datu[iii])\n",
    "                temp_acc_seq.append(datu[iii+1])\n",
    "            else:\n",
    "                dup.append(datu[iii])\n",
    "        return temp_acc_seq,len(dup)\n",
    "               \n",
    "    def ml_sl(self):\n",
    "        acc_seq=[]\n",
    "        for k in range(len(self.data)):\n",
    "            if self.data[k].startswith('>'):\n",
    "                acc_seq.append(self.data[k])\n",
    "                join_=0\n",
    "                for l in range(k+1,len(self.data)):\n",
    "                    if self.data[l].startswith('>') == False:\n",
    "                        join_+=1\n",
    "                    else:\n",
    "                        break\n",
    "                acc_seq.append(''.join(self.data[k+1:k+1+join_]))\n",
    "        return acc_seq\n",
    "\n",
    "class Pseaac:\n",
    "    def __init__(self,filename):\n",
    "        self.filename=filename\n",
    "    def collect(self,lamb,w,pro,nf):\n",
    "        val=[]\n",
    "        val.append(['#']+[ea for ea in keys.values()]+['\\u03BB'+str(eac+1) for eac in range(lamb)])\n",
    "        for e_seq in range(len(seq)):# single sequence taken for test\n",
    "            q=self.pse(seq[e_seq],lamb,w,pro,nf)\n",
    "            tem=acc[e_seq].split(' ')[0][1:]\n",
    "            val.append([acc[e_seq]]+q)\n",
    "        df=pd.DataFrame(val[1:],columns=val[0])\n",
    "#         print('PseAAC feature have been extracted!!!')\n",
    "        return df\n",
    "        \n",
    "    def pse(self,data,lamb,w,pro,nf):\n",
    "        thet=self.theta(data,lamb,pro)\n",
    "        deno=1+(w*sum(thet.values()))\n",
    "        p=[]\n",
    "        if nf==1:\n",
    "            norm=(len(data))\n",
    "        else:\n",
    "            norm=1\n",
    "        for u in range(1,21+lamb):\n",
    "            if u>=1 and u<=20:\n",
    "    #             print(u,'natural')\n",
    "                num=data.count(keys[u])/norm # frequency\n",
    "                p.append(num/deno)\n",
    "            elif u>=21 and u<=20+lamb:\n",
    "    #             print(u,'pseudo')\n",
    "                num=w*thet[u-20]\n",
    "                p.append(num/deno)\n",
    "        return p\n",
    "    \n",
    "    def theta(self,data,lamb,pro):\n",
    "        the={}\n",
    "        for u in range(1,lamb+1):\n",
    "            the[u]=(1/(len(data)-u))*self.rel_cal(data,u,pro)\n",
    "        return the\n",
    "    \n",
    "    def rel_cal(self,data,v,pro):\n",
    "        tem=[]\n",
    "        for u in range(len(data)-v):\n",
    "            te=[]\n",
    "            for u1 in pro:\n",
    "                x=((values[u1][data[u]])-(values[u1][data[u+v]]))**2\n",
    "                te.append(x)\n",
    "    #             print(u1,u,u+v,data[u],data[u+v],x)\n",
    "            tem.append(sum(te)/len(pro))\n",
    "        return sum(tem)\n",
    "\n",
    "keys,values=Standard_values('D:/After_4_4_22/Project_1_Part_1/CAZy_23_6_22/7_98_hydrophobicity.csv').get_prop([1,2,3,4,5,6,20,60])\n",
    "# pseaac_data['Length']=stats.zscore(min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clustering Algorithm'''\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, mixture, manifold, decomposition, preprocessing,metrics\n",
    "import random\n",
    "from collections import Counter,defaultdict\n",
    "import copy\n",
    "\n",
    "class clustering:\n",
    "    rs=77\n",
    "#     rn=random.randint(1,99)\n",
    "    def __init__(self,folder,data,n,cat):\n",
    "        self.x=data.iloc[:,1:].values\n",
    "        self.y=data.iloc[:,0]\n",
    "        self.folder=folder\n",
    "        self.cat=cat\n",
    "        self.anno_label={0:'acc',1:'prtn',2:'ec',3:'org',4:'species',5:'ghf'}\n",
    "        temp=[i.split('$')[cat[0]] for i in self.y]\n",
    "        lab=list(set(temp))\n",
    "        self.true_lab=[lab.index(j) for j in temp]\n",
    "        try:\n",
    "            n.isalpha()\n",
    "            self.n=len(lab)\n",
    "        except AttributeError:\n",
    "            self.n=n\n",
    "        self.temp1=','.join([f'{k}:{v}' for k,v in dict(Counter(temp)).items()])\n",
    "        \n",
    "    def kmeans(self):\n",
    "        start = time.time()\n",
    "        self.names = 'km'\n",
    "        kmeans = cluster.KMeans(n_clusters=self.n,random_state=clustering.rs) # Number of clusters\n",
    "        self.labels = kmeans.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def affinity(self):\n",
    "        start = time.time()\n",
    "        self.names = 'apc'\n",
    "        apc = cluster.AffinityPropagation(random_state=clustering.rs)\n",
    "        self.labels = apc.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def meanshift(self):\n",
    "        start = time.time()\n",
    "        self.names = 'ms'\n",
    "        ms = cluster.MeanShift()\n",
    "        self.labels = ms.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def spectral(self):\n",
    "        start = time.time()\n",
    "        self.names = 'spec'\n",
    "        spectral = cluster.SpectralClustering(n_clusters=self.n,affinity='nearest_neighbors',assign_labels=\"discretize\",random_state=clustering.rs) # Number of clusters\n",
    "        self.labels = spectral.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def agglomerative(self):\n",
    "        start = time.time()\n",
    "        self.names = 'agglo'\n",
    "        agglo = cluster.AgglomerativeClustering(n_clusters=self.n) # Number of clusters\n",
    "        self.labels = agglo.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def dbscan(self):\n",
    "        start = time.time()\n",
    "        self.names = 'dbs'\n",
    "        new_x=preprocessing.StandardScaler().fit_transform(self.x)\n",
    "        dbs = cluster.DBSCAN()\n",
    "        self.labels = dbs.fit_predict(new_x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def optics(self):\n",
    "        start = time.time()\n",
    "        self.names = 'opt'\n",
    "        opt = cluster.OPTICS()\n",
    "        self.labels = opt.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def gaussian(self):\n",
    "        start = time.time()\n",
    "        self.names = 'gm'\n",
    "        gm = mixture.GaussianMixture(n_components=self.n,random_state=clustering.rs) # Number of Clusters\n",
    "        self.labels = gm.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def birch(self):\n",
    "        start = time.time()\n",
    "        self.names = 'bir'\n",
    "        brc = cluster.Birch() # Number of clusters\n",
    "        self.labels = brc.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "\n",
    "    def label_save(self):\n",
    "        dfout = pd.DataFrame({'Accession': self.y,  'predicted': self.labels, 'expected':self.true_lab})\n",
    "        try:\n",
    "            os.mkdir(self.folder)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        dfout.to_csv(f'{self.folder}\\ML_{self.names}_{len(set(self.labels))}.txt',sep='\\t', index=False)\n",
    "        self.file()\n",
    "        return self.analysis()\n",
    "    \n",
    "    def analysis(self):\n",
    "        value=metrics.fowlkes_mallows_score(self.true_lab,self.labels)\n",
    "        tot_val=[self.names,self.anno_label[self.cat[0]],str(lambda_value),str(round(value,3)),str(self.n),str(len(set(self.labels))),self.temp1,str(len(self.true_lab))]\n",
    "        return tot_val\n",
    "    \n",
    "    def file(self):\n",
    "        try:\n",
    "            os.mkdir(f'{self.folder}/table')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        dd=defaultdict(list)\n",
    "        for i in range(len(self.labels)):\n",
    "            dd[self.labels[i]].append(self.y[i])\n",
    "        self.excel(dict(sorted(dd.items())))\n",
    "        \n",
    "    def excel(self,anno):\n",
    "        all_anno={}\n",
    "        for i in self.cat:\n",
    "            temp={}\n",
    "            for j,k in anno.items():\n",
    "                te=[]\n",
    "                for l in k:\n",
    "                    te.append(l.split('$')[i])\n",
    "                temp[j]=dict(Counter(te))\n",
    "            df=pd.DataFrame(temp).fillna(0).astype(int)\n",
    "            df.loc['Total']=df.sum(axis=0)\n",
    "            df.loc[:,'Total']=df.sum(axis=1)\n",
    "            df.to_excel(f'{self.folder}/table/{self.names}_{len(set(self.labels))}_{self.anno_label[i]}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ec_data:\n",
    "    def __init__(self,file):\n",
    "        self.data=[]\n",
    "        with open(file,'r') as inpt:\n",
    "            for i in inpt:\n",
    "                self.data.append(i.rstrip())\n",
    "\n",
    "    def cazy(self,dom,typ):\n",
    "        temp=[]\n",
    "        for i in self.data:\n",
    "            temp.append(i.split('$')[1].split(' '))\n",
    "        return self.domain(temp,dom,typ)\n",
    "    \n",
    "    def domain(self,ec_n,n,include):\n",
    "        single,multi=[],[]\n",
    "        for i in ec_n:\n",
    "            if len(i)<=n:\n",
    "                single.extend(i)\n",
    "            else:\n",
    "                multi.append(i)\n",
    "        print('Number of single domain:',len(single))\n",
    "        print(f'Number of multi domain (>{n}):',len(multi))\n",
    "        if include=='m':\n",
    "            for j in multi:\n",
    "                for k in j:\n",
    "                    single.append(k)\n",
    "            print('After including multi domains:',len(single))\n",
    "        return single\n",
    "    \n",
    "class analysis:\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        \n",
    "    def non_kegg_count(self):\n",
    "        temp=Counter(self.data)\n",
    "        return self.specific_ec(dict(temp))\n",
    "    \n",
    "    def kegg_count(self,label):\n",
    "        temp=defaultdict(int)\n",
    "        for j,k in zip(self.data,label):\n",
    "            temp[j]+=int(k)\n",
    "        return self.specific_ec(dict(temp))\n",
    "    \n",
    "    def specific_ec(self,dat):\n",
    "        caazy=[]\n",
    "        for u,v in dat.items():\n",
    "            if '3.2.1.' in u:\n",
    "                try:\n",
    "                    caazy.append([u,int(u.split('.')[-1])])\n",
    "                except ValueError:\n",
    "                    print(u)\n",
    "        cc_ec=[x[0] for x in sorted(caazy, key=lambda x:x[1])]\n",
    "        return cc_ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_filter:\n",
    "    def __init__(self,ac,se):\n",
    "        self.ac=ac\n",
    "        self.se=se\n",
    "    def present_absent(self,typ,data):\n",
    "        print('Filter has been applied....')\n",
    "        new_acc,new_seq=[],[]\n",
    "        not_acc,not_seq=[],[]\n",
    "        for i in range(len(self.ac)):\n",
    "            if self.ac[i].split('$')[-1] in data:\n",
    "                new_acc.append(self.ac[i])\n",
    "                new_seq.append(self.se[i])\n",
    "            else:\n",
    "                not_acc.append(self.ac[i])\n",
    "                not_seq.append(self.se[i])\n",
    "        if typ=='present':\n",
    "            return new_acc,new_seq\n",
    "        elif typ=='absent':\n",
    "            return not_acc,not_seq\n",
    "        else:\n",
    "            print('Check the spelling!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace is applied because \"\" are coming after opening the files in excel\n",
    "class selected_GH:\n",
    "    def __init__(self,file):\n",
    "        self.data={}\n",
    "        with open(file,'r') as inpt:\n",
    "            for i in inpt:\n",
    "                self.data[i.split('\\t')[0]]={j.split(':')[0]:int(j.split(':')[1]) if j.startswith('GH') else 0 for j in i.split('\\t')[7].replace('\"','').split(',')}\n",
    "        del self.data['EC_number']\n",
    "    \n",
    "    def output(self):\n",
    "        return self.data\n",
    "\n",
    "selected_data=selected_GH('EC_18_14_21_17.txt').output()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3.2.1.8': {'GH10': 392, 'GH11': 396},\n",
       " '3.2.1.14': {'GH18': 496, 'GH19': 196},\n",
       " '3.2.1.21': {'GH1': 361, 'GH3': 208},\n",
       " '3.2.1.17': {'GH22': 142, 'GH23': 53, 'GH24': 38, 'GH25': 90}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class balance_data:\n",
    "    def __init__(self,ac,se):\n",
    "        self.ac=ac\n",
    "        self.se=se\n",
    "        \n",
    "    def sizing(self,size_data):\n",
    "        new_ac,new_se=[],[]\n",
    "        min_size=min(list(size_data.values()))\n",
    "        for i in size_data:\n",
    "            temp_ac,temp_se=[],[]\n",
    "            for j in range(len(self.ac)):\n",
    "                if i==self.ac[j].split('$')[-1]:\n",
    "                    temp_ac.append(self.ac[j])\n",
    "                    temp_se.append(self.se[j])\n",
    "            new_cord=random.sample([m for m in range(len(temp_ac))],k=min_size)\n",
    "            for n in new_cord:\n",
    "                new_ac.append(temp_ac[n])\n",
    "                new_se.append(temp_se[n])\n",
    "        print('The data has been balanced to equal size.....')\n",
    "        return new_ac,new_se "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hits for BAA31551.1\n",
      "No hits for NP_149279.1\n",
      "No hits for NP_149217.1\n",
      "No hits for AAC98123.1\n",
      "No hits for ABI49937.1\n",
      "No hits for AAC98140.1\n",
      "No hits for CAA82319.1\n",
      "No hits for NP_242986.1\n",
      "No hits for ZP_08160171.1\n",
      "No hits for ZP_08159615.1\n",
      "No hits for ACR61562.1\n",
      "No hits for BAA19777.1\n",
      "No hits for ADQ57411.1\n",
      "No hits for YP_003851606.1\n",
      "No hits for AAZ56824.1\n",
      "No hits for AAZ56956.1\n",
      "No hits for CAD48748.1\n",
      "No hits for NP_227886.1\n",
      "No hits for NP_227877.1\n",
      "No hits for AAD32593.1\n",
      "No hits for NP_644553.1\n",
      "No hits for AAP31839.1\n",
      "No hits for EAA78230.1\n",
      "No hits for XP_002470423.1\n",
      "No hits for XP_365543.1\n",
      "No hits for BAA89465.1\n",
      "No hits for NP_241765.1\n",
      "No hits for CAD65888.1\n",
      "No hits for ZP_08160424.1\n",
      "No hits for ZP_08158180.1\n",
      "No hits for ZP_08157788.1\n",
      "No hits for NP_624448.1\n",
      "No hits for NP_626540.1\n",
      "No hits for BAA19778.1\n",
      "No hits for AAZ55251.1\n",
      "No hits for ABA39289.1\n",
      "No hits for ABM55503.1\n",
      "No hits for EAA73188.1\n",
      "No hits for XP_383800.1\n",
      "No hits for XP_368051.1\n",
      "No hits for AAD37441.1\n",
      "No hits for A44594\n",
      "No hits for NP_149282.1\n",
      "No hits for ZP_03013482.1\n",
      "No hits for ZP_03013476.1\n",
      "Sequence has been collected for 3.2.1.8....\n",
      "Filter has been applied....\n",
      "Feature has been extracted for 3.2.1.8....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:246: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  warnings.warn(\"Affinity propagation did not converge, this model \"\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_optics.py:803: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  ratio = reachability_plot[:-1] / reachability_plot[1:]\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_birch.py:647: ConvergenceWarning: Number of subclusters found (1) by Birch is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering has been done for 3.2.1.8....\n",
      "No hits for NP_279794.1\n",
      "No hits for NP_578962.1\n",
      "No hits for NP_578963.1\n",
      "No hits for NP_578962.1\n",
      "No hits for NP_833450.1\n",
      "No hits for AAU21943.1\n",
      "No hits for NP_902605.1\n",
      "No hits for AGD94964.1\n",
      "No hits for NP_814153.1\n",
      "No hits for NP_241782.1\n",
      "No hits for NP_268107.1\n",
      "No hits for CBW16119.1\n",
      "No hits for ABV39247.1\n",
      "No hits for ABV40327.1\n",
      "No hits for ABV41826.1\n",
      "No hits for ABV42574.1\n",
      "No hits for NP_630126.1\n",
      "No hits for NP_629155.1\n",
      "No hits for NP_629515.1\n",
      "No hits for NP_625711.1\n",
      "No hits for AAZ54618.1\n",
      "No hits for NP_232428.1\n",
      "No hits for AAA83586.1\n",
      "No hits for EAL00460.1\n",
      "No hits for ABC59330.1\n",
      "No hits for AGX26690.1\n",
      "No hits for AAH36339.1\n",
      "No hits for AAH47336.1\n",
      "No hits for CAH70802.1\n",
      "No hits for CAH70803.1\n",
      "No hits for CAH70804.1\n",
      "No hits for CAI19263.1\n",
      "No hits for CAI19265.1\n",
      "No hits for CAI19266.1\n",
      "No hits for NP_068569.1\n",
      "No hits for BAB25878.1\n",
      "No hits for BAB90566.1\n",
      "No hits for BAB91759.1\n",
      "No hits for ABB97081.1\n",
      "No hits for AAV58834.1\n",
      "No hits for BAA31200.1\n",
      "No hits for AAC96628.1\n",
      "No hits for AAC97066.2\n",
      "No hits for NP_048613.1\n",
      "No hits for AAC96549.2\n",
      "No hits for NP_048529.1\n",
      "No hits for AAC96549.2\n",
      "No hits for NP_048529.1\n",
      "No hits for NP_631319.1\n",
      "No hits for CAB57191.1\n",
      "No hits for NP_733504.1\n",
      "No hits for AAG21643.1\n",
      "No hits for NP_187856.1\n",
      "No hits for NP_566426.1\n",
      "No hits for AAB64320.1\n",
      "No hits for AAB64318.1\n",
      "No hits for AAB58239.1\n",
      "No hits for CAD41539.1\n",
      "Sequence has been collected for 3.2.1.14....\n",
      "Filter has been applied....\n",
      "Feature has been extracted for 3.2.1.14....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:246: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  warnings.warn(\"Affinity propagation did not converge, this model \"\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_optics.py:803: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  ratio = reachability_plot[:-1] / reachability_plot[1:]\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_birch.py:647: ConvergenceWarning: Number of subclusters found (1) by Birch is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering has been done for 3.2.1.14....\n",
      "No hits for NP_578171.1\n",
      "No hits for NP_142340.1\n",
      "No hits for ABP70047.1\n",
      "No hits for YP_066184.1\n",
      "No hits for NP_242789.1\n",
      "No hits for ABR73190.1\n",
      "No hits for NP_386997.1\n",
      "No hits for NP_625353.1\n",
      "No hits for NP_631601.1\n",
      "No hits for AAZ54975.1\n",
      "No hits for ADI56259.1\n",
      "No hits for NP_936184.1\n",
      "No hits for AEE33889.1\n",
      "No hits for NP_176375.1\n",
      "No hits for NP_187303.1\n",
      "No hits for AAG12895.1\n",
      "No hits for NP_198505.1\n",
      "No hits for AAA83309.1\n",
      "No hits for AAC68766.1\n",
      "No hits for NP_497558.1\n",
      "No hits for ABI34907.1\n",
      "No hits for ABI34907.2\n",
      "No hits for ACD65509.1\n",
      "No hits for EAA26947.1\n",
      "No hits for XP_322216.1\n",
      "No hits for BAB86071.1\n",
      "No hits for BAC06894.1\n",
      "No hits for BAF07003.1\n",
      "No hits for CAE05491.1\n",
      "No hits for CAE03398.1\n",
      "No hits for CAE01908.1\n",
      "No hits for CAE01909.1\n",
      "No hits for CAE01911.1\n",
      "No hits for AAA84906.2\n",
      "No hits for AAT85322.1\n",
      "No hits for AAL89551.1\n",
      "No hits for NP_812226.1\n",
      "No hits for YP_002352162.1\n",
      "No hits for ZP_01543735.1\n",
      "No hits for NP_227841.1\n",
      "No hits for XP_330872.1\n",
      "No hits for XP_324309.1\n",
      "No hits for XP_360965.1\n",
      "No hits for XP_364573.1\n",
      "No hits for AAL69548.2\n",
      "No hits for CAP58431.1\n",
      "No hits for ABR57325.1\n",
      "No hits for EAK85129.1\n",
      "No hits for NP_012272.1\n",
      "Sequence has been collected for 3.2.1.21....\n",
      "Filter has been applied....\n",
      "Feature has been extracted for 3.2.1.21....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:246: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  warnings.warn(\"Affinity propagation did not converge, this model \"\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:245: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_optics.py:803: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  ratio = reachability_plot[:-1] / reachability_plot[1:]\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_birch.py:647: ConvergenceWarning: Number of subclusters found (1) by Birch is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering has been done for 3.2.1.21....\n",
      "No hits for AAC00558.1\n",
      "No hits for ABK34500.1\n",
      "No hits for XP_046047.1\n",
      "No hits for NP_815238.1\n",
      "No hits for NP_420976.1\n",
      "No hits for NP_845781.1\n",
      "No hits for NP_845154.1\n",
      "No hits for NP_347193.1\n",
      "No hits for NP_562230.1\n",
      "No hits for NP_815667.1\n",
      "No hits for NP_814147.1\n",
      "No hits for BAB45607.1\n",
      "Sequence has been collected for 3.2.1.17....\n",
      "Filter has been applied....\n",
      "Feature has been extracted for 3.2.1.17....\n",
      "Clustering has been done for 3.2.1.17....\n",
      "Mission completed in 122.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_optics.py:803: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  ratio = reachability_plot[:-1] / reachability_plot[1:]\n",
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_birch.py:647: ConvergenceWarning: Number of subclusters found (1) by Birch is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# '1' stands for single domain, and 'm' stands for to include mutli domain in single domain sequences\n",
    "\n",
    "######## Chnage the folder name before running the programm ############\n",
    "main_file='EC_18_14_21_17'\n",
    "os.mkdir(main_file)\n",
    "    \n",
    "# a_cazy=ec_data('D:/After_4_4_22/Project_1_Part_1/CAZy_23_6_22/cazy_char_10_6_22.txt').cazy(1,'s')\n",
    "# c_cazy=analysis(a_cazy).non_kegg_count()\n",
    "\n",
    "# for y,z in selected_data.items():\n",
    "#     print(y,list(z.keys()))\n",
    "\n",
    "t1=time.perf_counter()\n",
    "total_data=[]\n",
    "total_rm_cazy,total_rm_genbank={},{}\n",
    "value_error,no_entry=[],[]\n",
    "multi_gh={}\n",
    "\n",
    "for each,peach in selected_data.items():\n",
    "    ec_number=each\n",
    "    gh_family='all'\n",
    "    cazy_acc_seq,rm_cazy,rm_genbank=In_data.EC_GH(ec_number,gh_family) # write all to fetch all the EC number or all the GH family\n",
    "    total_rm_cazy[each],total_rm_genbank[each]=rm_cazy,rm_genbank\n",
    "    acc,seq,min_len,x_aa,dupli,multi_gh[each]=Sequence(cazy_acc_seq).output()\n",
    "    print(f'Sequence has been collected for {each}....')\n",
    "    \n",
    "######## Filtered applied #################################\n",
    "\n",
    "    acc,seq=add_filter(acc,seq).present_absent('present',list(peach.keys()))\n",
    "    \n",
    "######## Taking equal size GH families #####################\n",
    "\n",
    "#     acc,seq=balance_data(acc,seq).sizing(peach)\n",
    "    \n",
    "    min_lamb=30\n",
    "    try:\n",
    "        if min(min_len)<min_lamb:\n",
    "            lambda_value=min(min_len)-1\n",
    "        else:\n",
    "            lambda_value=min_lamb\n",
    "    except ValueError:\n",
    "        print(each,': doesnt have sequences')\n",
    "        no_entry.append(each)\n",
    "        continue\n",
    "    pseaac_data=Pseaac(f'PAAC_{ec_number}_{gh_family}_L{lambda_value}.txt').collect(lambda_value,0.05,[60,2,3],1)\n",
    "    print(f'Feature has been extracted for {each}....')\n",
    "    ec_=ec_number.replace('.','_')\n",
    "    # In the below statement 'auto' means it takes automatic clusters based on number labels given, you can also choose any number.\n",
    "    clust=clustering(f'{main_file}/{gh_family}_{ec_}',pseaac_data,'auto',[5]) # 0:'acc',1:'prtn',2:'ec',3:'org',4:'species',5:'ghf'\n",
    "    try:\n",
    "        km=clust.kmeans()\n",
    "        apc=clust.affinity()\n",
    "        ms=clust.meanshift()\n",
    "        spec=clust.spectral()\n",
    "        agglo=clust.agglomerative()\n",
    "        dbs=clust.dbscan()\n",
    "        opt=clust.optics()\n",
    "        gm=clust.gaussian()\n",
    "        bir=clust.birch()\n",
    "    except ValueError:\n",
    "        print(f'{each} has {len(acc)} samples which is less than 5 min_samples ')\n",
    "        value_error.append([each,len(acc)])\n",
    "        continue\n",
    "    print(f'Clustering has been done for {each}....')\n",
    "    all_clust={'km':km,'apc':apc,'ms':ms,'spec':spec,'agglo':agglo,'dbs':dbs,'opt':opt,'gm':gm,'bir':bir}\n",
    "    for aa in all_clust.values():\n",
    "        bb=[each]+aa+[str(len(rm_cazy)),str(len(rm_genbank))]+[str(x_aa)]+[str(dupli),str(len(multi_gh[each]))]\n",
    "        total_data.append('$'.join(bb))\n",
    "t2=time.perf_counter()\n",
    "print('Mission completed in',round(t2-t1,3),'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "titl=['EC_number','Method','label_type','lambda','FMI','ex_groups','pred_groups','distribution','total','CAZy_partial','Fasta_partial','X_aa','Duplicates','Multi_GH_fam']\n",
    "outpt=open('Summary_ec_number_18_14_21_17.txt','w')\n",
    "outpt.write('$'.join(titl)+'\\n')\n",
    "for line in tqdm(total_data):\n",
    "    outpt.write(line+'\\n')\n",
    "outpt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
