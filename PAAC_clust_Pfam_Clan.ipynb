{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import re\n",
    "# 1: Hydrophobicity, 2: Hydrophilicity, 3: mass, 4: pk1, 5:pk2, 6:pi, 20: 14 scale, 60: Tanford\n",
    "class Standard_values:\n",
    "    def __init__(self,filename):\n",
    "        self.data=[]\n",
    "        with open(filename,'r') as inpt:\n",
    "            for each in inpt:\n",
    "                self.data.append(each.rstrip().split(','))\n",
    "        del self.data[0]\n",
    "        \n",
    "    def get_prop(self,prop):\n",
    "        got_prop,amino_acid={},{}\n",
    "        for each in prop:\n",
    "            got_prop[each]=self.properties(each)\n",
    "        d=1\n",
    "        for each in got_prop[1].keys():\n",
    "            amino_acid[d]=each\n",
    "            d+=1\n",
    "        return amino_acid,got_prop\n",
    "    \n",
    "    def properties(self, val):\n",
    "        temp,norm={},{}\n",
    "        for each in self.data:\n",
    "            temp[each[0]]=float(each[val])\n",
    "        relative=stats.zscore(np.array(list(temp.values())))\n",
    "        for a,b in zip(temp.keys(),relative):\n",
    "            norm[a]=b\n",
    "        return norm\n",
    "\n",
    "class Sequence:\n",
    "    def __init__(self,filename):\n",
    "        self.data=[]\n",
    "        if type(filename)==str:\n",
    "            with open(filename,'r') as inpt:\n",
    "                for each in inpt:\n",
    "                    self.data.append(each.rstrip())\n",
    "        else:\n",
    "            self.data=filename\n",
    "                       \n",
    "    def output(self):\n",
    "        a,s,l=[],[],[]\n",
    "        unusual=0\n",
    "        for ele in self.ml_sl():\n",
    "            if ele.startswith('>'):\n",
    "                a.append(ele)\n",
    "            else:\n",
    "                if re.search('[UZOBJX]',ele.upper()):\n",
    "#                     print(a[-1])\n",
    "                    del a[-1]\n",
    "                    unusual+=1\n",
    "                    continue\n",
    "                l.append(len(ele))\n",
    "                s.append(ele.upper())\n",
    "#         print('The length of the smallest sequence:',min(l))\n",
    "#         print('Sequence with \"X\" present:',unusual)\n",
    "        return a,s,l,unusual\n",
    "             \n",
    "    def ml_sl(self):\n",
    "        acc_seq=[]\n",
    "        for k in range(len(self.data)):\n",
    "            if self.data[k].startswith('>'):\n",
    "                acc_seq.append(self.data[k])\n",
    "                join_=0\n",
    "                for l in range(k+1,len(self.data)):\n",
    "                    if self.data[l].startswith('>') == False:\n",
    "                        join_+=1\n",
    "                    else:\n",
    "                        break\n",
    "                acc_seq.append(''.join(self.data[k+1:k+1+join_]))\n",
    "        return acc_seq\n",
    "\n",
    "class Pseaac:\n",
    "    def __init__(self,filename):\n",
    "        self.filename=filename\n",
    "    def collect(self,lamb,w,pro,nf):\n",
    "        val=[]\n",
    "        val.append(['#']+[ea for ea in keys.values()]+['\\u03BB'+str(eac+1) for eac in range(lamb)])\n",
    "        for e_seq in range(len(seq)):# single sequence taken for test\n",
    "            q=self.pse(seq[e_seq],lamb,w,pro,nf)\n",
    "            tem=acc[e_seq].split(' ')[0][1:]\n",
    "            val.append([acc[e_seq]]+q)\n",
    "        df=pd.DataFrame(val[1:],columns=val[0])\n",
    "#         print('PseAAC feature have been extracted!!!')\n",
    "        return df\n",
    "        \n",
    "    def pse(self,data,lamb,w,pro,nf):\n",
    "        thet=self.theta(data,lamb,pro)\n",
    "        deno=1+(w*sum(thet.values()))\n",
    "        p=[]\n",
    "        if nf==1:\n",
    "            norm=(len(data))\n",
    "        else:\n",
    "            norm=1\n",
    "        for u in range(1,21+lamb):\n",
    "            if u>=1 and u<=20:\n",
    "    #             print(u,'natural')\n",
    "                num=data.count(keys[u])/norm # frequency\n",
    "                p.append(num/deno)\n",
    "            elif u>=21 and u<=20+lamb:\n",
    "    #             print(u,'pseudo')\n",
    "                num=w*thet[u-20]\n",
    "                p.append(num/deno)\n",
    "        return p\n",
    "    \n",
    "    def theta(self,data,lamb,pro):\n",
    "        the={}\n",
    "        for u in range(1,lamb+1):\n",
    "            the[u]=(1/(len(data)-u))*self.rel_cal(data,u,pro)\n",
    "        return the\n",
    "    \n",
    "    def rel_cal(self,data,v,pro):\n",
    "        tem=[]\n",
    "        for u in range(len(data)-v):\n",
    "            te=[]\n",
    "            for u1 in pro:\n",
    "                x=((values[u1][data[u]])-(values[u1][data[u+v]]))**2\n",
    "                te.append(x)\n",
    "    #             print(u1,u,u+v,data[u],data[u+v],x)\n",
    "            tem.append(sum(te)/len(pro))\n",
    "        return sum(tem)\n",
    "\n",
    "keys,values=Standard_values('D:/After_4_4_22/data/CAZy_23_6_22/7_98_hydrophobicity.csv').get_prop([1,2,3,4,5,6,20,60])\n",
    "# pseaac_data['Length']=stats.zscore(min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clustering Algorithm'''\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, mixture, manifold, decomposition, preprocessing,metrics\n",
    "import random\n",
    "from collections import Counter,defaultdict\n",
    "import copy\n",
    "\n",
    "class clustering:\n",
    "    rs=77\n",
    "#     rn=random.randint(1,99)\n",
    "    def __init__(self,folder,data,n,cat):\n",
    "        self.x=data.iloc[:,1:].values\n",
    "        self.y=data.iloc[:,0]\n",
    "        self.folder=folder\n",
    "        self.cat=cat\n",
    "        self.anno_label={0:'acc',1:'pfam',2:'ec',3:'org',4:'species',5:'ghf'}\n",
    "        temp=[i.split('$')[cat[0]] for i in self.y]\n",
    "        lab=list(set(temp))\n",
    "        self.true_lab=[lab.index(j) for j in temp]\n",
    "        try:\n",
    "            n.isalpha()\n",
    "            self.n=len(lab)\n",
    "        except AttributeError:\n",
    "            self.n=n\n",
    "        self.temp1=','.join([f'{k}:{v}' for k,v in dict(Counter(temp)).items()])\n",
    "        \n",
    "    def kmeans(self):\n",
    "        start = time.time()\n",
    "        self.names = 'km'\n",
    "        kmeans = cluster.KMeans(n_clusters=self.n,random_state=clustering.rs) # Number of clusters\n",
    "        self.labels = kmeans.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def affinity(self):\n",
    "        start = time.time()\n",
    "        self.names = 'apc'\n",
    "        apc = cluster.AffinityPropagation(random_state=clustering.rs)\n",
    "        self.labels = apc.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def meanshift(self):\n",
    "        start = time.time()\n",
    "        self.names = 'ms'\n",
    "        ms = cluster.MeanShift()\n",
    "        self.labels = ms.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def spectral(self):\n",
    "        start = time.time()\n",
    "        self.names = 'spec'\n",
    "        spectral = cluster.SpectralClustering(n_clusters=self.n,assign_labels=\"discretize\",random_state=clustering.rs) # Number of clusters\n",
    "        self.labels = spectral.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def agglomerative(self):\n",
    "        start = time.time()\n",
    "        self.names = 'agglo'\n",
    "        agglo = cluster.AgglomerativeClustering(n_clusters=self.n) # Number of clusters\n",
    "        self.labels = agglo.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def dbscan(self):\n",
    "        start = time.time()\n",
    "        self.names = 'dbs'\n",
    "        new_x=preprocessing.StandardScaler().fit_transform(self.x)\n",
    "        dbs = cluster.DBSCAN()\n",
    "        self.labels = dbs.fit_predict(new_x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def optics(self):\n",
    "        start = time.time()\n",
    "        self.names = 'opt'\n",
    "        opt = cluster.OPTICS()\n",
    "        self.labels = opt.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def gaussian(self):\n",
    "        start = time.time()\n",
    "        self.names = 'gm'\n",
    "        gm = mixture.GaussianMixture(n_components=self.n,random_state=clustering.rs) # Number of Clusters\n",
    "        self.labels = gm.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "        \n",
    "    def birch(self):\n",
    "        start = time.time()\n",
    "        self.names = 'bir'\n",
    "        brc = cluster.Birch() # Number of clusters\n",
    "        self.labels = brc.fit_predict(self.x)\n",
    "        end = time.time()\n",
    "        self.t = round((end-start),3)\n",
    "        return self.label_save()\n",
    "\n",
    "    def label_save(self):\n",
    "        dfout = pd.DataFrame({'Accession': self.y,  'predicted': self.labels, 'expected':self.true_lab})\n",
    "        try:\n",
    "            os.mkdir(self.folder)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        dfout.to_csv(f'{self.folder}\\ML_{self.names}_{len(set(self.labels))}.txt',sep='\\t', index=False)\n",
    "        self.file()\n",
    "        return self.analysis()\n",
    "    \n",
    "    def analysis(self):\n",
    "        value=metrics.fowlkes_mallows_score(self.true_lab,self.labels)\n",
    "        tot_val=[self.names,self.anno_label[self.cat[0]],str(lambda_value),str(round(value,3)),str(self.n),str(len(set(self.labels))),self.temp1,str(len(self.true_lab))]\n",
    "        return tot_val\n",
    "    \n",
    "    def file(self):\n",
    "        try:\n",
    "            os.mkdir(f'{self.folder}/table')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        dd=defaultdict(list)\n",
    "        for i in range(len(self.labels)):\n",
    "            dd[self.labels[i]].append(self.y[i])\n",
    "        self.excel(dict(sorted(dd.items())))\n",
    "        \n",
    "    def excel(self,anno):\n",
    "        all_anno={}\n",
    "        for i in self.cat:\n",
    "            temp={}\n",
    "            for j,k in anno.items():\n",
    "                te=[]\n",
    "                for l in k:\n",
    "                    te.append(l.split('$')[i])\n",
    "                temp[j]=dict(Counter(te))\n",
    "            df=pd.DataFrame(temp).fillna(0).astype(int)\n",
    "            df.loc['Total']=df.sum(axis=0)\n",
    "            df.loc[:,'Total']=df.sum(axis=1)\n",
    "            df.to_excel(f'{self.folder}/table/{self.names}_{len(set(self.labels))}_{self.anno_label[i]}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class fetch_seq:\n",
    "    def __init__(self,file):\n",
    "        self.acc,self.seq=[],[]\n",
    "        with open(f'{file}','r') as inpt:\n",
    "            for i in inpt:\n",
    "                if i.startswith('>'):\n",
    "                    self.acc.append(i.rstrip()+'$'+file.split('\\\\')[1].split('_')[0])\n",
    "                else:\n",
    "                    self.seq.append(i.rstrip())\n",
    "    def show(self,n):\n",
    "        new_acc,new_seq=[],[]\n",
    "        if type(n)==int:\n",
    "            temp=random.sample([i for i in range(len(self.acc))],k=n)\n",
    "            for j in temp:\n",
    "                new_acc.append(self.acc[j])\n",
    "                new_seq.append(self.seq[j])\n",
    "        else:\n",
    "            new_acc=self.acc\n",
    "            new_seq=self.seq\n",
    "        data=[]\n",
    "        for k in range(len(new_acc)):\n",
    "            data.append(new_acc[k])\n",
    "            data.append(new_seq[k])\n",
    "        return data   \n",
    "# a=fetch_seq(ii).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "all_data=[]\n",
    "sample_size=10\n",
    "for ii in glob.glob('CL0037_pfam/PF*.txt'):# Beware when you add the folders as labels are taken from file name\n",
    "    jj=ii.split('.')[0].split('_')[-1]\n",
    "    if int(jj)>=sample_size: # Sample size\n",
    "        if int(jj)>=sample_size:\n",
    "            samp=10\n",
    "        else:\n",
    "            samp='all'\n",
    "        all_data.extend(fetch_seq(ii).show(samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence has been collected for CL0037....\n",
      "Feature has been extracted for CL0037....\n",
      "Clustering has been done for CL0037....\n",
      "Mission completed in 4.287 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rakes\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\cluster\\_birch.py:647: ConvergenceWarning: Number of subclusters found (1) by Birch is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t1=time.perf_counter()\n",
    "total_data=[]\n",
    "for each in ['CL0037']:\n",
    "    acc,seq,min_len,x_aa=Sequence(all_data).output()\n",
    "    print(f'Sequence has been collected for {each}....')\n",
    "    min_lamb=30\n",
    "    try:\n",
    "        if min(min_len)<min_lamb:\n",
    "            lambda_value=min(min_len)\n",
    "        else:\n",
    "            lambda_value=min_lamb\n",
    "    except ValueError:\n",
    "        print(each,': doesnt have sequences')\n",
    "        continue\n",
    "    pseaac_data=Pseaac(f'PAAC_{each}_{sample_size}_L{lambda_value}.txt').collect(lambda_value,0.05,[60,2,3],1)\n",
    "    print(f'Feature has been extracted for {each}....')\n",
    "#     In the below statement 'auto' means it takes automatic clusters based on number labels given, you can also choose any number.\n",
    "    clust=clustering(f'{each}_{sample_size}',pseaac_data,'auto',[1]) # 0:'acc',1:'prtn',2:'ec',3:'org',4:'species',5:'ghf'\n",
    "    try:\n",
    "        km=clust.kmeans()\n",
    "        apc=clust.affinity()\n",
    "        ms=clust.meanshift()\n",
    "        spec=clust.spectral()\n",
    "        agglo=clust.agglomerative()\n",
    "        dbs=clust.dbscan()\n",
    "        opt=clust.optics()\n",
    "        gm=clust.gaussian()\n",
    "        bir=clust.birch()\n",
    "    except ValueError:\n",
    "        print(f'{each} has {len(acc)} samples which is less than 5 min_samples ')\n",
    "        value_error.append([each,len(acc)])\n",
    "        continue\n",
    "    print(f'Clustering has been done for {each}....')\n",
    "    all_clust={'km':km,'apc':apc,'ms':ms,'spec':spec,'agglo':agglo,'dbs':dbs,'opt':opt,'gm':gm,'bir':bir}\n",
    "    for aa in all_clust.values():\n",
    "        bb=[each]+aa+[str(x_aa)]\n",
    "        total_data.append('$'.join(bb))\n",
    "t2=time.perf_counter()\n",
    "print('Mission completed in',round(t2-t1,3),'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "titl=['Clan','Method','label_type','lambda','FMI','ex_groups','pred_groups','distribution','total','X_aa']\n",
    "outpt=open(f'PFam_number_cluster_Clan_{sample_size}.txt','w')\n",
    "outpt.write('$'.join(titl)+'\\n')\n",
    "for line in tqdm(total_data):\n",
    "    outpt.write(line+'\\n')\n",
    "outpt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
